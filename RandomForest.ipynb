{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO4ywP4C3FTTAXwMplw+S1E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"f8Vj_DcScRgq","executionInfo":{"status":"ok","timestamp":1698583433557,"user_tz":-180,"elapsed":7,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}}},"outputs":[],"source":["import numpy as np\n","from collections import Counter\n","\n","class Node:\n","    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n","        self.feature = feature\n","        self.threshold = threshold\n","        self.left = left\n","        self.right = right\n","        self.value = value\n","\n","    def is_leaf_node(self):\n","        return self.value is not None\n","\n","\n","class DecisionTree:\n","    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n","        self.min_samples_split=min_samples_split\n","        self.max_depth=max_depth\n","        self.n_features=n_features\n","        self.root=None\n","\n","    def fit(self, X, y):\n","        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n","        self.root = self._grow_tree(X, y)\n","\n","    def _grow_tree(self, X, y, depth=0):\n","        n_samples, n_feats = X.shape\n","        n_labels = len(np.unique(y))\n","\n","        # check the stopping criteria\n","        if (depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split):\n","            leaf_value = self._most_common_label(y)\n","            return Node(value=leaf_value)\n","\n","        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n","\n","        # find the best split\n","        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n","\n","        # create child nodes\n","        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n","        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n","        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n","        return Node(best_feature, best_thresh, left, right)\n","\n","\n","    def _best_split(self, X, y, feat_idxs):\n","        best_gain = -1\n","        split_idx, split_threshold = None, None\n","\n","        for feat_idx in feat_idxs:\n","            X_column = X[:, feat_idx]\n","            thresholds = np.unique(X_column)\n","\n","            for thr in thresholds:\n","                # calculate the information gain\n","                gain = self._information_gain(y, X_column, thr)\n","\n","                if gain > best_gain:\n","                    best_gain = gain\n","                    split_idx = feat_idx\n","                    split_threshold = thr\n","\n","        return split_idx, split_threshold\n","\n","\n","    def _information_gain(self, y, X_column, threshold):\n","        # parent entropy\n","        parent_entropy = self._entropy(y)\n","\n","        # create children\n","        left_idxs, right_idxs = self._split(X_column, threshold)\n","\n","        if len(left_idxs) == 0 or len(right_idxs) == 0:\n","            return 0\n","\n","        # calculate the weighted avg. entropy of children\n","        n = len(y)\n","        n_l, n_r = len(left_idxs), len(right_idxs)\n","        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n","        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n","\n","        # calculate the IG\n","        information_gain = parent_entropy - child_entropy\n","        return information_gain\n","\n","    def _split(self, X_column, split_thresh):\n","        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n","        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n","        return left_idxs, right_idxs\n","\n","    def _entropy(self, y):\n","        hist = np.bincount(y)\n","        ps = hist / len(y)\n","        return -np.sum([p * np.log(p) for p in ps if p>0])\n","\n","\n","    def _most_common_label(self, y):\n","        counter = Counter(y)\n","        value = counter.most_common(1)[0][0]\n","        return value\n","\n","    def predict(self, X):\n","        return np.array([self._traverse_tree(x, self.root) for x in X])\n","\n","    def _traverse_tree(self, x, node):\n","        if node.is_leaf_node():\n","            return node.value\n","\n","        if x[node.feature] <= node.threshold:\n","            return self._traverse_tree(x, node.left)\n","        return self._traverse_tree(x, node.right)\n","\n","\n"]},{"cell_type":"code","source":["class RandomForest:\n","  def __init__(self,n_tree = 10,  min_samples_split=2, max_depth=10, n_features=None):\n","        self.min_samples_split=min_samples_split\n","        self.max_depth=max_depth\n","        self.n_features=n_features\n","        self.n_tree = n_tree\n","\n","  def fit(self,X,y):\n","    n_samples = X.shape[0]\n","\n","\n","\n","\n","    # itarete and create trees\n","    self.trees = []\n","    for i in range(self.n_tree):\n","      tree = DecisionTree(max_depth=self.max_depth,\n","                            min_samples_split=self.min_samples_split,\n","                            n_features=self.n_features)\n","\n","      idxs = self._get_random_samp_with_replac(n_samples)\n","\n","      X = X[idxs,:]\n","      y =y[idxs]\n","      tree.fit(X,y)\n","\n","      self.trees.append(tree)\n","\n","      print(\"----------------- \", i , \" out of \", self.n_tree)\n","\n","\n","  def predict(self,X):\n","    preds = [tree.predict(X) for tree in self.trees]\n","    all_pred = np.swapaxes(preds,0,1)\n","    out = [self._most_common_label(t_pred) for t_pred in all_pred]\n","    return out\n","\n","  def _most_common_label(self, y):\n","        counter = Counter(y)\n","        value = counter.most_common(1)[0][0]\n","        return value\n","\n","\n","  def _get_random_samp_with_replac(self,n_samples):\n","    idxs = np.random.choice(n_samples,n_samples,replace=True)\n","    return idxs"],"metadata":{"id":"rrzrXrGedjqb","executionInfo":{"status":"ok","timestamp":1698585448707,"user_tz":-180,"elapsed":279,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","bc = datasets.load_breast_cancer()\n","X,y = bc.data,bc.target\n","X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42,shuffle=True)"],"metadata":{"id":"PQSVasxQfoKh","executionInfo":{"status":"ok","timestamp":1698585365775,"user_tz":-180,"elapsed":2,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["clf = RandomForest(n_tree=100)\n","clf.fit(X_train,y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UrYbNcWggIg_","executionInfo":{"status":"ok","timestamp":1698585557658,"user_tz":-180,"elapsed":10538,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}},"outputId":"b0d20114-15ae-47a0-980d-ee173cb9face"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["-----------------  0  out of  100\n","-----------------  1  out of  100\n","-----------------  2  out of  100\n","-----------------  3  out of  100\n","-----------------  4  out of  100\n","-----------------  5  out of  100\n","-----------------  6  out of  100\n","-----------------  7  out of  100\n","-----------------  8  out of  100\n","-----------------  9  out of  100\n","-----------------  10  out of  100\n","-----------------  11  out of  100\n","-----------------  12  out of  100\n","-----------------  13  out of  100\n","-----------------  14  out of  100\n","-----------------  15  out of  100\n","-----------------  16  out of  100\n","-----------------  17  out of  100\n","-----------------  18  out of  100\n","-----------------  19  out of  100\n","-----------------  20  out of  100\n","-----------------  21  out of  100\n","-----------------  22  out of  100\n","-----------------  23  out of  100\n","-----------------  24  out of  100\n","-----------------  25  out of  100\n","-----------------  26  out of  100\n","-----------------  27  out of  100\n","-----------------  28  out of  100\n","-----------------  29  out of  100\n","-----------------  30  out of  100\n","-----------------  31  out of  100\n","-----------------  32  out of  100\n","-----------------  33  out of  100\n","-----------------  34  out of  100\n","-----------------  35  out of  100\n","-----------------  36  out of  100\n","-----------------  37  out of  100\n","-----------------  38  out of  100\n","-----------------  39  out of  100\n","-----------------  40  out of  100\n","-----------------  41  out of  100\n","-----------------  42  out of  100\n","-----------------  43  out of  100\n","-----------------  44  out of  100\n","-----------------  45  out of  100\n","-----------------  46  out of  100\n","-----------------  47  out of  100\n","-----------------  48  out of  100\n","-----------------  49  out of  100\n","-----------------  50  out of  100\n","-----------------  51  out of  100\n","-----------------  52  out of  100\n","-----------------  53  out of  100\n","-----------------  54  out of  100\n","-----------------  55  out of  100\n","-----------------  56  out of  100\n","-----------------  57  out of  100\n","-----------------  58  out of  100\n","-----------------  59  out of  100\n","-----------------  60  out of  100\n","-----------------  61  out of  100\n","-----------------  62  out of  100\n","-----------------  63  out of  100\n","-----------------  64  out of  100\n","-----------------  65  out of  100\n","-----------------  66  out of  100\n","-----------------  67  out of  100\n","-----------------  68  out of  100\n","-----------------  69  out of  100\n","-----------------  70  out of  100\n","-----------------  71  out of  100\n","-----------------  72  out of  100\n","-----------------  73  out of  100\n","-----------------  74  out of  100\n","-----------------  75  out of  100\n","-----------------  76  out of  100\n","-----------------  77  out of  100\n","-----------------  78  out of  100\n","-----------------  79  out of  100\n","-----------------  80  out of  100\n","-----------------  81  out of  100\n","-----------------  82  out of  100\n","-----------------  83  out of  100\n","-----------------  84  out of  100\n","-----------------  85  out of  100\n","-----------------  86  out of  100\n","-----------------  87  out of  100\n","-----------------  88  out of  100\n","-----------------  89  out of  100\n","-----------------  90  out of  100\n","-----------------  91  out of  100\n","-----------------  92  out of  100\n","-----------------  93  out of  100\n","-----------------  94  out of  100\n","-----------------  95  out of  100\n","-----------------  96  out of  100\n","-----------------  97  out of  100\n","-----------------  98  out of  100\n","-----------------  99  out of  100\n"]}]},{"cell_type":"code","source":["sum(clf.predict(X_test)==y_test)/len(y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MwyZV2Dhgg6-","executionInfo":{"status":"ok","timestamp":1698585569410,"user_tz":-180,"elapsed":269,"user":{"displayName":"Mahmoud Atia Ead Atia","userId":"11781797524477583668"}},"outputId":"0ab50237-447f-43d3-fd67-f814a0790f99"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9440559440559441"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":[],"metadata":{"id":"udE5vOvQgrd-"},"execution_count":null,"outputs":[]}]}